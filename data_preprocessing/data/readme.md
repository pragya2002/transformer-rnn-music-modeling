ğŸ“ Data Folder

This folder contains (or links to) the preprocessed symbolic music dataset used for all Transformer and RNN experiments in this project.
The full dataset is too large for GitHub, so it is hosted on Google Drive.

ğŸ“¥ Download Dataset (Required)

ğŸ”— Preprocessed ABC + Tokenized Dataset (Google Drive) in compressed form
https://drive.google.com/drive/folders/11i9D8xs85fGnUEKKNWVCvfL6wzyUrAi3?usp=sharing

Download and extract this into the data/ directory of the repository.

ğŸ“¦ Contents (after extraction)
```text
data/
â”œâ”€â”€ raw_midi/              # (NOT in repo) original Lakh MIDI files (.mid)
â”‚   â””â”€â”€ ...                # 176k MIDI files, ~1.6 GB compressed / larger extracted
â”‚
â”œâ”€â”€ abc/                   # (NOT in repo) MIDI converted to ABC
â”‚   â”œâ”€â”€ 000001.abc
â”‚   â”œâ”€â”€ 000002.abc
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ 175926.abc
â”‚
â”œâ”€â”€ corpus/
â”‚   â””â”€â”€ abc_corpus.txt     # large concatenated ABC text corpus (~3.6 GB)
â”‚
â””â”€â”€ tokenized/
    â”œâ”€â”€ vocab.json         # character-level vocabulary (stoi / itos, 100 chars)
    â”œâ”€â”€ all_ids.npy        # int32 token sequence for 150M characters
    â”œâ”€â”€ train_ids.npy      # 100M tokens
    â”œâ”€â”€ val_ids.npy        # 25M tokens
    â””â”€â”€ test_ids.npy       # 25M tokens
```
ğŸ“ Notes

Dataset originates from Lakh MIDI (176,581 files), converted to ABC.

Cleaned and processed using the scripts in data_preprocessing/.

Used for all training, scaling, and generation experiments.
